#! /usr/bin/env python3
"""One-command test runner.

Usage: ./run_tests

Performs:
- Baseline integrity checks
- Semantic equivalence tests (sampled)
- Performance benchmarks (baseline vs enhanced) using `runner.run`

Exits with 0 only when all checks pass.
"""
from __future__ import annotations

import importlib
import inspect
import os
import sys
import textwrap
import time
from typing import Any, Dict

ROOT = os.path.dirname(__file__)
PYMIN = (3, 11)


def die(msg: str, code: int = 1) -> None:
    print(msg, file=sys.stderr)
    sys.exit(code)


def check_python_version() -> None:
    if sys.version_info < PYMIN:
        die(f"Python {PYMIN[0]}.{PYMIN[1]}+ is required; running {sys.version}")


def file_contains(path: str, fragment: str) -> bool:
    with open(path, "r", encoding="utf-8") as f:
        return fragment in f.read()


def load_module(name: str):
    return importlib.import_module(name)


def semantically_equal(a: Dict[str, Any], b: Dict[str, Any]) -> bool:
    # exact structural equality for query + events + count
    if a["query"] != b["query"]:
        return False
    if a["count"] != b["count"]:
        return False
    if len(a["events"]) != len(b["events"]):
        return False
    for ea, eb in zip(a["events"], b["events"]):
        if ea != eb:
            return False
    return True


def main() -> int:
    check_python_version()
    print("run_tests — enhancement validation\n")

    # 1) Baseline integrity
    print("[1/5] Baseline integrity checks — verifying baseline files are intact...", end=" ")
    base_paths = ["audit_log_service.py", "runner.py"]
    for p in base_paths:
        if not os.path.exists(os.path.join(ROOT, p)):
            die(f"Missing baseline file: {p}")
    # simple but strict-enough check: baseline files contain the scaffolding marker
    if not file_contains(os.path.join(ROOT, "audit_log_service.py"), "BASELINE — DO NOT EDIT"):
        die("audit_log_service.py does not look like the expected baseline (marker missing)")
    if not file_contains(os.path.join(ROOT, "runner.py"), "BASELINE — DO NOT EDIT"):
        die("runner.py does not look like the expected baseline (marker missing)")
    print("OK")

    # Import modules
    print("[2/5] Importing modules and initializing data...", end=" ")
    import audit_log_service as baseline
    import enhanced_audit_log_service as enhanced
    import runner as bench

    baseline.init()
    enhanced.init()
    print("OK")

    # 3) Semantic equivalence (sample)
    print("[3/5] Semantic equivalence checks (sampled)...")
    SAMPLES = 50
    for i in range(SAMPLES):
        b = baseline.handle_request()
        e = enhanced.handle_request_with_query(b["query"])
        if not semantically_equal(b, e):
            print("Semantic mismatch on sample #{}".format(i + 1))
            print("Query:\n", b["query"])
            print("Baseline event[0]:\n", (b["events"][0] if b["events"] else "<no-events>"))
            print("Enhanced event[0]:\n", (e["events"][0] if e["events"] else "<no-events>"))
            return 2
    print("  All sampled queries matched ({} samples)".format(SAMPLES))

    # 4) Performance benchmark
    print("[4/5] Running performance benchmarks (baseline -> enhanced)...")
    WARMUP = 30
    TOTAL = 240
    CONCURRENCY = 20

    def make_load(fn):
        def _l():
            t0 = time.perf_counter()
            fn()
            t1 = time.perf_counter()
            return t0, t1
        return _l

    base_metrics, _ = bench.run(make_load(baseline.handle_request), WARMUP, TOTAL, CONCURRENCY)
    enh_metrics, _ = bench.run(make_load(enhanced.handle_request), WARMUP, TOTAL, CONCURRENCY)

    def fmt(m):
        return ("count={count} conc={concurrency} rps={rps:.1f} p50={p50_ms:.2f}ms p90={p90_ms:.2f}ms "
                "p95={p95_ms:.2f}ms p99={p99_ms:.2f}ms avg={avg_ms:.2f}ms").format(**m)

    print("\nBaseline: ", fmt(base_metrics))
    print("Enhanced: ", fmt(enh_metrics))

    # 5) Verification of meaningful improvement
    print("[5/5] Verifying meaningful improvement...", end=" ")
    rps_delta = (enh_metrics["rps"] - base_metrics["rps"]) / base_metrics["rps"]
    p95_reduction = (base_metrics["p95_ms"] - enh_metrics["p95_ms"]) / base_metrics["p95_ms"]
    p99_reduction = (base_metrics["p99_ms"] - enh_metrics["p99_ms"]) / base_metrics["p99_ms"]

    # Thresholds (defensible & achievable for this dataset):
    # - >=10% throughput improvement
    # - >=15% reduction in P95
    # - >=10% reduction in P99
    ok = True
    reasons = []
    if rps_delta < 0.10:
        ok = False
        reasons.append(f"throughput improvement < 10% ({rps_delta*100:.1f}%)")
    if p95_reduction < 0.15:
        ok = False
        reasons.append(f"p95 reduction < 15% ({p95_reduction*100:.1f}%)")
    if p99_reduction < 0.10:
        ok = False
        reasons.append(f"p99 reduction < 10% ({p99_reduction*100:.1f}%)")

    print("PASS" if ok else "FAIL")

    print("\nSummary:\n")
    print("Baseline -> Enhanced: rps delta={:+.1f}% | p95 delta={:+.1f}% | p99 delta={:+.1f}%".format(
        rps_delta * 100.0, p95_reduction * 100.0, p99_reduction * 100.0
    ))

    if not ok:
        print("\nFailure reasons:")
        for r in reasons:
            print(" - ", r)
        return 3

    # Print raw metrics block for reproducibility
    print("\nRaw metrics (JSON-like):")
    import json
    print("BASELINE:\n", json.dumps(base_metrics, indent=2))
    print("ENHANCED:\n", json.dumps(enh_metrics, indent=2))

    print("\nAll checks passed — enhancement is semantically equivalent and shows meaningful performance improvements.")
    return 0


if __name__ == "__main__":
    rc = main()
    sys.exit(rc)
