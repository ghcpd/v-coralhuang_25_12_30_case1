## **Enhancement – Improve Efficiency and Robustness of an Existing Backend Feature (Implementation Required)**

### Fixed Baseline Contract (Read Carefully)

The baseline implementation:

* Uses **OFFSET-based pagination**
* Stores audit event payloads in a **single disk-backed JSONL file**
* For each returned event:

  * Performs a disk seek + read
  * Decodes JSON payload
* Sort order is fixed:

  ```
  created_at DESC, id DESC
  ```

**Baseline files are immutable**:

* `audit_log_service.py`
* `runner.py`

You must not modify them in any way.

---

### Dataset Characteristics (Do Not Change)

The dataset and query distribution are part of the contract:

* Rows: **50,000**
* Page range: `page ∈ [1, 2000]`
* Page size: `{10, 20, 50}`
* Payload size: JSON with a 256–1024 char text field
* Payloads must be read from disk for each returned event

You must not regenerate data with a different size, structure, or storage model.

---

### Your Task

Enhance the existing implementation by improving execution efficiency and robustness under concurrency, **while strictly preserving semantics**.

Typical enhancement directions include (but are not limited to):

* Cursor / Keyset pagination
* More efficient query patterns
* Improved resource lifecycle and concurrency management

This is an **enhancement** task, not a rewrite, refactor, or feature change.

---

### Hard Constraints (Mandatory)

* Semantic equivalence is mandatory

  * Same fields
  * Same ordering
  * Same result counts
  * Same disk-backed payload reads
* Baseline files must remain unchanged
* Enhancements must be implemented via **new files only**
* **Python standard library only** (no third-party dependencies)

---

### Required Deliverables (Strict)

#### 1. Enhanced Implementation

* One or more **new source files** implementing the enhanced logic
* Baseline files must remain runnable and unchanged

#### 2. One-Command Automated Test Runner

You must provide an executable script named:

```
run_tests
```

The script must:

* Be runnable via:

  ```bash
  ./run_tests
  ```
* Automatically perform:

  * Baseline integrity verification
  * Semantic equivalence checks (baseline vs enhanced)
  * Performance benchmarking using the provided `runner.py`
* Print results directly to the terminal, including:

  * Throughput (RPS)
  * P50 / P90 / P95 / P99 latency
* Exit with:

  * Code `0` only if **all checks pass**
  * Non-zero code if any check fails

Manual or multi-step testing is not acceptable.

---

#### 3. Reproducible Environment Specification (Required)

You must provide a **reusable, deterministic environment setup**, either by:

* Clearly documenting the required Python version and setup steps in `README.md`, or
* Providing minimal environment configuration files (e.g. `.env.example` if needed)

A reviewer must be able to run `./run_tests` on a clean machine without manual intervention.

---

#### 4. README.md (Mandatory)

You must provide a `README.md` that includes **at minimum**:

* A brief description of the baseline bottlenecks
* The enhancement strategy and why it improves performance
* Confirmation that semantics are preserved
* Environment setup instructions
* How to run:

  ```bash
  ./run_tests
  ```
* The **raw output** of a successful `run_tests` execution

Free-form or purely theoretical documentation is not sufficient.

---

### Acceptance Criteria

A submission is considered successful only if:

* Baseline files are unchanged
* `./run_tests` runs successfully with exit code `0`
* Semantic equivalence checks pass
* P95 / P99 latency improves meaningfully compared to baseline
* Throughput improves meaningfully compared to baseline
* Results are reproducible using the documented environment

---

### Important Reminder

This is not a theoretical exercise.

Treat this as a **production enhancement under SLA pressure**:

* The system is already live
* Behavior must not change
* Improvements must be measurable, reproducible, and defensible
